{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c00b933-49ed-42df-8a72-e7fe25de54c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === INR-based HSI Compression with SIREN (Patch-based) ===#\n",
    "\n",
    "########## ===== Hyperspectral Image Compression Using Implicit Neural Representation =====#########\n",
    "   ######### =========== By Shima Rezasoltani, Faisal Z. Qureshi =================##############\n",
    "      ############# ============== Ontario Tech University ====================== ##########\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import scipy.io as sio\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import mean_squared_error\n",
    "from torch.quantization import quantize_dynamic\n",
    "import math\n",
    "import time\n",
    "from scipy import linalg\n",
    "\n",
    "try:\n",
    "    from skimage.metrics import mean_squared_error as mse\n",
    "except ImportError:\n",
    "    from skimage.measure import compare_mse as mse\n",
    "\n",
    "# Set device\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE)\n",
    "\n",
    "# === Load and Normalize PaviaU ===\n",
    "data = sio.loadmat('/home/data/Fahad/HSI_datasets/PaviaU/PaviaU.mat')\n",
    "cube = data['paviaU']  # shape: [610, 340, 103]\n",
    "mean = cube.mean()\n",
    "std = cube.std()\n",
    "cube_norm = (cube - mean) / std  # Z-score normalization\n",
    "original_cube = cube_norm.copy()\n",
    "\n",
    "# === Patch Parameters ===\n",
    "PATCH_SIZE = 64\n",
    "OVERLAP = 0  # Optional overlap between patches\n",
    "H, W, C = cube_norm.shape\n",
    "\n",
    "# === Function to Create Patches ===\n",
    "def create_patches(image, patch_size, overlap=0):\n",
    "    patches = []\n",
    "    patch_coords = []\n",
    "    \n",
    "    # Calculate step size\n",
    "    step = patch_size - overlap\n",
    "    \n",
    "    # Pad the image if necessary\n",
    "    pad_h = (patch_size - H % step) % step\n",
    "    pad_w = (patch_size - W % step) % step\n",
    "    \n",
    "    padded_image = np.pad(image, ((0, pad_h), (0, pad_w), (0, 0)), mode='reflect')\n",
    "    padded_H, padded_W = padded_image.shape[:2]\n",
    "    \n",
    "    # Create patches\n",
    "    for i in range(0, padded_H - patch_size + 1, step):\n",
    "        for j in range(0, padded_W - patch_size + 1, step):\n",
    "            patch = padded_image[i:i+patch_size, j:j+patch_size]\n",
    "            \n",
    "            # Normalize patch coordinates to [-1, 1]\n",
    "            x_coords = np.linspace(-1 + (2*i)/padded_H, -1 + (2*(i+patch_size))/padded_H, patch_size)\n",
    "            y_coords = np.linspace(-1 + (2*j)/padded_W, -1 + (2*(j+patch_size))/padded_W, patch_size)\n",
    "            \n",
    "            grid_x, grid_y = np.meshgrid(x_coords, y_coords, indexing='ij')\n",
    "            coords = np.stack([grid_x, grid_y], axis=-1).reshape(-1, 2)\n",
    "            \n",
    "            patches.append(patch.reshape(-1, C))\n",
    "            patch_coords.append(coords)\n",
    "    \n",
    "    return patches, patch_coords, (pad_h, pad_w)\n",
    "\n",
    "# Create patches\n",
    "patches, patch_coords, padding = create_patches(cube_norm, PATCH_SIZE, OVERLAP)\n",
    "print(f\"Created {len(patches)} patches of size {PATCH_SIZE}x{PATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c77c485-57d2-4aa6-b7ab-03eb0b6cdd10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Define Sine Activation ===\n",
    "class Sine(nn.Module):\n",
    "    def __init__(self, w0=30):\n",
    "        super().__init__()\n",
    "        self.w0 = w0\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sin(self.w0 * x)\n",
    "\n",
    "# === SIREN Model ===\n",
    "class SIREN(nn.Module):\n",
    "    def __init__(self, in_dim=2, hidden_dim=512, out_dim=103, hidden_layers=4, w0=30):\n",
    "        super().__init__()\n",
    "        self.net = [nn.Linear(in_dim, hidden_dim), Sine(w0)]\n",
    "        for _ in range(hidden_layers):\n",
    "            self.net.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.net.append(Sine(w0))\n",
    "        self.net.append(nn.Linear(hidden_dim, out_dim))\n",
    "        self.model = nn.Sequential(*self.net)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# === Custom Weight Initialization ===\n",
    "def init_weights(m, w0=30, is_first=False):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        in_dim = m.weight.shape[1]\n",
    "        if is_first:\n",
    "            bound = 1 / in_dim\n",
    "        else:\n",
    "            bound = np.sqrt(6 / in_dim) / w0\n",
    "        with torch.no_grad():\n",
    "            m.weight.uniform_(-bound, bound)\n",
    "            m.bias.fill_(0)\n",
    "        \n",
    "\n",
    "# === Instantiate & Initialize Model ===\n",
    "model = SIREN(in_dim=2, hidden_dim=512, out_dim=C, hidden_layers=4).to(DEVICE).float()\n",
    "for i, layer in enumerate(model.model):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        init_weights(layer, w0=30, is_first=(i == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814ea4f0-1b4d-40a5-839c-20a0c0735b21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Training Setup ===\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "\n",
    "# === Training Loop ===\n",
    "epochs = 800\n",
    "batch_size = 18384  # Batch size within each patch\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    processed_patches = 0\n",
    "    \n",
    "    # Shuffle patch order\n",
    "    patch_order = np.random.permutation(len(patches))\n",
    "    \n",
    "    for patch_idx in patch_order:\n",
    "        # Get current patch data\n",
    "        current_coords = torch.tensor(patch_coords[patch_idx], dtype=torch.float32, device=DEVICE)\n",
    "        current_values = torch.tensor(patches[patch_idx], dtype=torch.float32, device=DEVICE)\n",
    "        \n",
    "        # Shuffle within patch\n",
    "        permutation = torch.randperm(current_coords.shape[0])\n",
    "        patch_loss = 0\n",
    "        \n",
    "        for i in range(0, current_coords.shape[0], batch_size):\n",
    "            idx = permutation[i:i+batch_size]\n",
    "            batch_coords = current_coords[idx]\n",
    "            batch_values = current_values[idx]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(batch_coords)\n",
    "            loss = loss_fn(preds, batch_values)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            patch_loss += loss.item()\n",
    "        \n",
    "        avg_patch_loss = patch_loss / (current_coords.shape[0] // batch_size + 1)\n",
    "        epoch_loss += avg_patch_loss\n",
    "        processed_patches += 1\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / processed_patches\n",
    "    scheduler.step(avg_epoch_loss)\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_epoch_loss < best_loss:\n",
    "        best_loss = avg_epoch_loss\n",
    "        torch.save(model.state_dict(), '/home/data/Fahad/models/best_INRpatch_model_weights4.pth')\n",
    "        print(f\"Saved new best model with loss: {best_loss:.6f}\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_epoch_loss:.6f} | LR: {optimizer.param_groups[0]['lr']:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5c4eca-da4c-45af-ad82-6f6e9ee277b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('/home/data/Fahad/models/best_INRpatch_model_weights4.pth'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef18a87-2c60-4c52-be5e-87a3276d4ca3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Model Size Calculation ===\n",
    "def get_model_size(model, quantization_bits=32):\n",
    "    \"\"\"Calculate model size in bits using state_dict (works for quantized models)\"\"\"\n",
    "    total_bits = 0\n",
    "    for name, param in model.state_dict().items():\n",
    "        if isinstance(param, torch.Tensor):\n",
    "            total_bits += param.numel() * quantization_bits\n",
    "    return total_bits\n",
    "\n",
    "\n",
    "# Calculate sizes\n",
    "original_size_bits = get_model_size(model, 32)\n",
    "# quantized_size_bits = get_model_size(quantized_model, 8)\n",
    "\n",
    "print(f\"Original model size: {original_size_bits / 8 / 1024:.2f} KB\")\n",
    "# print(f\"Quantized model size: {quantized_size_bits / 8 / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a78bdb-5b71-45a4-bb26-6f58a0c2da3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Compression Ratio and Bitrate in bpppc ===\n",
    "original_cube_size = H * W * C * 32  # Original uncompressed size in bits (float32)\n",
    "compression_ratio_original = original_cube_size / original_size_bits\n",
    "# compression_ratio_quantized = original_cube_size / quantized_size_bits\n",
    "\n",
    "# Bitrate in bpppc (bits per pixel per channel)\n",
    "bitrate_original = original_size_bits / (H * W * C)\n",
    "# bitrate_quantized = quantized_size_bits / (H * W * C)\n",
    "\n",
    "print(f\"\\nCompression Ratios:\")\n",
    "print(f\"Original model: {compression_ratio_original:.2f}:1\")\n",
    "# print(f\"Quantized model: {compression_ratio_quantized:.2f}:1\")\n",
    "\n",
    "print(f\"\\nBitrates (bpppc):\")\n",
    "print(f\"Original model: {bitrate_original:.4f} bpppc\")\n",
    "# print(f\"Quantized model: {bitrate_quantized:.6f} bpppc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a336231-8890-4c09-8d8f-f2ad3490cf0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Reconstruction Function ===\n",
    "def reconstruct_image(model, original_shape, patch_size, overlap, padding):\n",
    "    # Initialize reconstructed image\n",
    "    rec_image = np.zeros(original_shape)\n",
    "    count = np.zeros(original_shape[:2])  # To handle overlapping regions\n",
    "    \n",
    "    padded_H = original_shape[0] + padding[0]\n",
    "    padded_W = original_shape[1] + padding[1]\n",
    "    step = patch_size - overlap\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, padded_H - patch_size + 1, step):\n",
    "            for j in range(0, padded_W - patch_size + 1, step):\n",
    "                # Create coordinates for this patch\n",
    "                x_coords = np.linspace(-1 + (2*i)/padded_H, -1 + (2*(i+patch_size))/padded_H, patch_size)\n",
    "                y_coords = np.linspace(-1 + (2*j)/padded_W, -1 + (2*(j+patch_size))/padded_W, patch_size)\n",
    "                \n",
    "                grid_x, grid_y = np.meshgrid(x_coords, y_coords, indexing='ij')\n",
    "                coords = np.stack([grid_x, grid_y], axis=-1).reshape(-1, 2)\n",
    "                coords = torch.tensor(coords, dtype=torch.float32, device=DEVICE)\n",
    "                \n",
    "                # Predict patch values\n",
    "                pred = model(coords).cpu().numpy().reshape(patch_size, patch_size, -1)\n",
    "                \n",
    "                # Place in reconstructed image (handling overlap with averaging)\n",
    "                end_i = min(i+patch_size, original_shape[0])\n",
    "                end_j = min(j+patch_size, original_shape[1])\n",
    "                \n",
    "                rec_image[i:end_i, j:end_j] += pred[:end_i-i, :end_j-j]\n",
    "                count[i:end_i, j:end_j] += 1\n",
    "    \n",
    "    # Average overlapping regions\n",
    "    rec_image /= count[..., np.newaxis]\n",
    "    return rec_image\n",
    "\n",
    "# Reconstruct with both models\n",
    "print(\"\\nReconstructing with original model...\")\n",
    "reconstructed_original = reconstruct_image(model, cube_norm.shape, PATCH_SIZE, OVERLAP, padding)\n",
    "\n",
    "# print(\"Reconstructing with quantized model...\")\n",
    "# reconstructed_quantized = reconstruct_image(quantized_model, cube_norm.shape, PATCH_SIZE, OVERLAP, padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af72b3d-30cf-44ce-9ee6-d10f52e691ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For MS-SSIM\n",
    "try:\n",
    "    from skimage.metrics import structural_similarity as ms_ssim\n",
    "except ImportError:\n",
    "    # Fallback for older versions of skimage\n",
    "    try:\n",
    "        from skimage.measure import compare_msssim as ms_ssim\n",
    "    except ImportError:\n",
    "        ms_ssim = None\n",
    "\n",
    "# === Image Quality Metrics ===\n",
    "def calculate_psnr(original, reconstructed, data_range=1.0):\n",
    "    \"\"\"Calculate PSNR for each band and return average\"\"\"\n",
    "    psnrs = []\n",
    "    for c in range(original.shape[2]):\n",
    "        psnrs.append(psnr(original[..., c], reconstructed[..., c], data_range=data_range))\n",
    "    return np.mean(psnrs)\n",
    "\n",
    "\n",
    "def calculate_ms_ssim(original, reconstructed, data_range=1.0):\n",
    "    \"\"\"Calculate MS-SSIM for each band and return average\"\"\"\n",
    "    if ms_ssim is None:\n",
    "        raise ImportError(\"MS-SSIM not available in your skimage version. Requires scikit-image >= 0.19\")\n",
    "    \n",
    "    msssims = []\n",
    "    for c in range(original.shape[2]):\n",
    "        msssims.append(ms_ssim(original[..., c], reconstructed[..., c], data_range=data_range,\n",
    "                             channel_axis=None, win_size=7))\n",
    "    return np.mean(msssims)\n",
    "\n",
    "def calculate_sam(original, reconstructed):\n",
    "    \"\"\"Calculate Spectral Angle Mapper (SAM)\"\"\"\n",
    "    # Flatten spatial dimensions\n",
    "    orig_flat = original.reshape(-1, original.shape[2])\n",
    "    rec_flat = reconstructed.reshape(-1, reconstructed.shape[2])\n",
    "    \n",
    "    # Calculate dot product and magnitudes\n",
    "    dot_product = np.sum(orig_flat * rec_flat, axis=1)\n",
    "    orig_mag = np.sqrt(np.sum(orig_flat**2, axis=1))\n",
    "    rec_mag = np.sqrt(np.sum(rec_flat**2, axis=1))\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    mask = (orig_mag * rec_mag) > 0\n",
    "    cos_theta = np.zeros_like(dot_product)\n",
    "    cos_theta[mask] = dot_product[mask] / (orig_mag[mask] * rec_mag[mask])\n",
    "    \n",
    "    # Clamp to avoid numerical errors\n",
    "    cos_theta = np.clip(cos_theta, -1.0 + 1e-10, 1.0 - 1e-10)\n",
    "    angles = np.arccos(cos_theta)\n",
    "    \n",
    "    # Convert to degrees and return mean\n",
    "    return np.mean(np.rad2deg(angles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d626fede-ce8b-4199-85fb-2a07c0c66194",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Denormalize\n",
    "reconstructed_original = reconstructed_original * std + mean\n",
    "# reconstructed_quantized = reconstructed_quantized * std + mean\n",
    "\n",
    "# === Calculate Metrics ===\n",
    "print(\"\\n=== Quality Metrics for Original Model ===\")\n",
    "psnr_val = calculate_psnr(cube, reconstructed_original, data_range=cube.max()-cube.min())\n",
    "sam_val = calculate_sam(cube, reconstructed_original)\n",
    "ms_ssim_val = calculate_ms_ssim(cube, reconstructed_original)\n",
    "print(f\"PSNR: {psnr_val:.2f} dB\")\n",
    "print(f\"MS_SSIM: {ms_ssim_val:.4f}\")\n",
    "print(f\"SAM: {sam_val:.4f} degrees\")\n",
    "\n",
    "# print(\"\\n=== Quality Metrics for Quantized Model ===\")\n",
    "# psnr_val_q = calculate_psnr(cube, reconstructed_quantized, data_range=cube.max()-cube.min())\n",
    "# ssim_val_q = calculate_ssim(cube, reconstructed_quantized, data_range=cube.max()-cube.min())\n",
    "# sam_val_q = calculate_sam(cube, reconstructed_quantized)\n",
    "# print(f\"PSNR: {psnr_val_q:.2f} dB\")\n",
    "# print(f\"SSIM: {ssim_val_q:.4f}\")\n",
    "# print(f\"SAM: {sam_val_q:.2f} degrees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d134b31e-3ffe-48cb-a226-7546d1e3510f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
